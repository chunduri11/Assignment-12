{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment-12_pytorch_ResNet-18.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhzgPF5H_KMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2,2,2,2])\n",
        "\n",
        "def ResNet34():\n",
        "    return ResNet(BasicBlock, [3,4,6,3])\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3,4,6,3])\n",
        "\n",
        "def ResNet101():\n",
        "    return ResNet(Bottleneck, [3,4,23,3])\n",
        "\n",
        "def ResNet152():\n",
        "    return ResNet(Bottleneck, [3,8,36,3])\n",
        "\n",
        "\n",
        "def test():\n",
        "    net = ResNet18()\n",
        "    y = net(torch.randn(1,3,32,32))\n",
        "    print(y.size())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toTdT1LHECoT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "530d1f66-6bb4-4341-c2b4-72dd8a51304b"
      },
      "source": [
        "'''Train CIFAR10 with PyTorch.'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.optim.lr_scheduler import CyclicLR, ReduceLROnPlateau\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "lr = 0.1\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "best_acc = 0  # best test accuracy\n",
        "start_epoch = 0  # start from epoch 0 or last checkpoint epoch\n",
        "\n",
        "# Data\n",
        "print('==> Preparing data..')\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Preparing data..\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ0cvYyO92CO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "39759842-f5a1-4007-d996-d75b3686c460"
      },
      "source": [
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# Model\n",
        "print('==> Building model..')\n",
        "net = ResNet18()\n",
        "net = net.to(device)\n",
        "if device == 'cuda':\n",
        "    net = torch.nn.DataParallel(net)\n",
        "    cudnn.benchmark = True\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==> Building model..\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0kon2MG9LO3",
        "colab_type": "text"
      },
      "source": [
        "#### Using \"ReduceLROnPlateau\" as LR scheduler\n",
        "\n",
        "We are using simple LR scheduler Reduce on Platue, which reduces LR which helps in exploring local deeper bottoms in the visinity. We can see that It takes long time to reach reasonable accuracy of 90%, compared to Cyclical LR.\n",
        "\n",
        "In subsiquent training changes in this notebook itself, we use CyclicalLR and CLR with long step size (number of iterations per step), we can get better accuracy and in very less number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBBCe4u3pWxM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4ef4f22a-722c-4003-d9db-2d6cab41030a"
      },
      "source": [
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min')\n",
        "\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in net.parameters())\n",
        "pytorch_total_params_trainable = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
        "print(\"pytorch_total_params\",pytorch_total_params)\n",
        "print(\"pytorch_total_params_trainable\",pytorch_total_params_trainable)\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time = time.time()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print(\"total train iters \", len(trainloader), '| time: %.3f sec Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % ((time.time()-start_time), train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "start_time = time.time()\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "        scheduler.step(test_loss) \n",
        "        print(\"total test iters \", len(testloader), '| time: %.3f sec Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % ((time.time()-start_time), test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    \n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+100):\n",
        "    train(epoch)\n",
        "    test(epoch)    "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pytorch_total_params 11173962\n",
            "pytorch_total_params_trainable 11173962\n",
            "\n",
            "Epoch: 0\n",
            "total train iters  391 | time: 40.596 sec Loss: 1.601 | Acc: 40.410% (20205/50000)\n",
            "total test iters  100 | time: 43.436 sec Loss: 1.485 | Acc: 47.780% (4778/10000)\n",
            "\n",
            "Epoch: 1\n",
            "total train iters  391 | time: 41.917 sec Loss: 1.093 | Acc: 60.540% (30270/50000)\n",
            "total test iters  100 | time: 88.056 sec Loss: 1.327 | Acc: 58.300% (5830/10000)\n",
            "\n",
            "Epoch: 2\n",
            "total train iters  391 | time: 41.199 sec Loss: 0.849 | Acc: 70.124% (35062/50000)\n",
            "total test iters  100 | time: 132.054 sec Loss: 1.092 | Acc: 62.800% (6280/10000)\n",
            "\n",
            "Epoch: 3\n",
            "total train iters  391 | time: 41.415 sec Loss: 0.701 | Acc: 75.816% (37908/50000)\n",
            "total test iters  100 | time: 176.223 sec Loss: 0.808 | Acc: 72.640% (7264/10000)\n",
            "\n",
            "Epoch: 4\n",
            "total train iters  391 | time: 41.258 sec Loss: 0.617 | Acc: 78.842% (39421/50000)\n",
            "total test iters  100 | time: 220.250 sec Loss: 0.665 | Acc: 77.330% (7733/10000)\n",
            "\n",
            "Epoch: 5\n",
            "total train iters  391 | time: 41.320 sec Loss: 0.571 | Acc: 80.268% (40134/50000)\n",
            "total test iters  100 | time: 264.286 sec Loss: 0.748 | Acc: 75.130% (7513/10000)\n",
            "\n",
            "Epoch: 6\n",
            "total train iters  391 | time: 41.380 sec Loss: 0.539 | Acc: 81.438% (40719/50000)\n",
            "total test iters  100 | time: 308.443 sec Loss: 0.679 | Acc: 77.730% (7773/10000)\n",
            "\n",
            "Epoch: 7\n",
            "total train iters  391 | time: 41.281 sec Loss: 0.509 | Acc: 82.670% (41335/50000)\n",
            "total test iters  100 | time: 352.520 sec Loss: 0.630 | Acc: 79.000% (7900/10000)\n",
            "\n",
            "Epoch: 8\n",
            "total train iters  391 | time: 41.191 sec Loss: 0.487 | Acc: 83.248% (41624/50000)\n",
            "total test iters  100 | time: 396.465 sec Loss: 0.794 | Acc: 74.830% (7483/10000)\n",
            "\n",
            "Epoch: 9\n",
            "total train iters  391 | time: 41.174 sec Loss: 0.470 | Acc: 83.770% (41885/50000)\n",
            "total test iters  100 | time: 440.373 sec Loss: 0.714 | Acc: 77.220% (7722/10000)\n",
            "\n",
            "Epoch: 10\n",
            "total train iters  391 | time: 41.162 sec Loss: 0.452 | Acc: 84.632% (42316/50000)\n",
            "total test iters  100 | time: 484.314 sec Loss: 0.804 | Acc: 74.820% (7482/10000)\n",
            "\n",
            "Epoch: 11\n",
            "total train iters  391 | time: 41.118 sec Loss: 0.440 | Acc: 84.786% (42393/50000)\n",
            "total test iters  100 | time: 528.167 sec Loss: 0.515 | Acc: 82.360% (8236/10000)\n",
            "\n",
            "Epoch: 12\n",
            "total train iters  391 | time: 41.146 sec Loss: 0.430 | Acc: 85.206% (42603/50000)\n",
            "total test iters  100 | time: 572.094 sec Loss: 0.634 | Acc: 79.440% (7944/10000)\n",
            "\n",
            "Epoch: 13\n",
            "total train iters  391 | time: 41.224 sec Loss: 0.417 | Acc: 85.778% (42889/50000)\n",
            "total test iters  100 | time: 616.064 sec Loss: 0.647 | Acc: 78.670% (7867/10000)\n",
            "\n",
            "Epoch: 14\n",
            "total train iters  391 | time: 41.218 sec Loss: 0.411 | Acc: 85.858% (42929/50000)\n",
            "total test iters  100 | time: 660.014 sec Loss: 0.675 | Acc: 77.830% (7783/10000)\n",
            "\n",
            "Epoch: 15\n",
            "total train iters  391 | time: 41.258 sec Loss: 0.407 | Acc: 86.120% (43060/50000)\n",
            "total test iters  100 | time: 703.995 sec Loss: 0.680 | Acc: 78.080% (7808/10000)\n",
            "\n",
            "Epoch: 16\n",
            "total train iters  391 | time: 41.206 sec Loss: 0.397 | Acc: 86.584% (43292/50000)\n",
            "total test iters  100 | time: 747.925 sec Loss: 0.604 | Acc: 79.770% (7977/10000)\n",
            "\n",
            "Epoch: 17\n",
            "total train iters  391 | time: 41.266 sec Loss: 0.391 | Acc: 86.576% (43288/50000)\n",
            "total test iters  100 | time: 791.944 sec Loss: 0.793 | Acc: 75.680% (7568/10000)\n",
            "\n",
            "Epoch: 18\n",
            "total train iters  391 | time: 41.225 sec Loss: 0.387 | Acc: 86.718% (43359/50000)\n",
            "total test iters  100 | time: 835.899 sec Loss: 0.638 | Acc: 79.130% (7913/10000)\n",
            "\n",
            "Epoch: 19\n",
            "total train iters  391 | time: 41.142 sec Loss: 0.383 | Acc: 86.930% (43465/50000)\n",
            "total test iters  100 | time: 879.756 sec Loss: 0.441 | Acc: 85.110% (8511/10000)\n",
            "\n",
            "Epoch: 20\n",
            "total train iters  391 | time: 41.086 sec Loss: 0.379 | Acc: 87.158% (43579/50000)\n",
            "total test iters  100 | time: 923.579 sec Loss: 0.494 | Acc: 83.290% (8329/10000)\n",
            "\n",
            "Epoch: 21\n",
            "total train iters  391 | time: 41.063 sec Loss: 0.379 | Acc: 87.060% (43530/50000)\n",
            "total test iters  100 | time: 967.363 sec Loss: 0.548 | Acc: 81.100% (8110/10000)\n",
            "\n",
            "Epoch: 22\n",
            "total train iters  391 | time: 41.072 sec Loss: 0.362 | Acc: 87.588% (43794/50000)\n",
            "total test iters  100 | time: 1011.167 sec Loss: 0.473 | Acc: 84.040% (8404/10000)\n",
            "\n",
            "Epoch: 23\n",
            "total train iters  391 | time: 41.093 sec Loss: 0.366 | Acc: 87.458% (43729/50000)\n",
            "total test iters  100 | time: 1054.966 sec Loss: 0.601 | Acc: 81.010% (8101/10000)\n",
            "\n",
            "Epoch: 24\n",
            "total train iters  391 | time: 41.137 sec Loss: 0.360 | Acc: 87.796% (43898/50000)\n",
            "total test iters  100 | time: 1098.841 sec Loss: 0.472 | Acc: 84.490% (8449/10000)\n",
            "\n",
            "Epoch: 25\n",
            "total train iters  391 | time: 41.133 sec Loss: 0.364 | Acc: 87.664% (43832/50000)\n",
            "total test iters  100 | time: 1142.676 sec Loss: 0.684 | Acc: 77.750% (7775/10000)\n",
            "\n",
            "Epoch: 26\n",
            "total train iters  391 | time: 41.123 sec Loss: 0.353 | Acc: 87.948% (43974/50000)\n",
            "total test iters  100 | time: 1186.530 sec Loss: 0.615 | Acc: 79.800% (7980/10000)\n",
            "\n",
            "Epoch: 27\n",
            "total train iters  391 | time: 41.199 sec Loss: 0.350 | Acc: 87.980% (43990/50000)\n",
            "total test iters  100 | time: 1230.476 sec Loss: 0.521 | Acc: 83.160% (8316/10000)\n",
            "\n",
            "Epoch: 28\n",
            "total train iters  391 | time: 41.233 sec Loss: 0.353 | Acc: 87.960% (43980/50000)\n",
            "total test iters  100 | time: 1274.448 sec Loss: 0.496 | Acc: 83.690% (8369/10000)\n",
            "\n",
            "Epoch: 29\n",
            "total train iters  391 | time: 41.228 sec Loss: 0.352 | Acc: 88.016% (44008/50000)\n",
            "total test iters  100 | time: 1318.444 sec Loss: 0.593 | Acc: 79.770% (7977/10000)\n",
            "\n",
            "Epoch: 30\n",
            "total train iters  391 | time: 41.250 sec Loss: 0.348 | Acc: 88.134% (44067/50000)\n",
            "total test iters  100 | time: 1362.443 sec Loss: 0.508 | Acc: 83.440% (8344/10000)\n",
            "\n",
            "Epoch: 31\n",
            "total train iters  391 | time: 41.248 sec Loss: 0.194 | Acc: 93.510% (46755/50000)\n",
            "total test iters  100 | time: 1406.446 sec Loss: 0.232 | Acc: 92.150% (9215/10000)\n",
            "\n",
            "Epoch: 32\n",
            "total train iters  391 | time: 41.239 sec Loss: 0.145 | Acc: 95.178% (47589/50000)\n",
            "total test iters  100 | time: 1450.425 sec Loss: 0.217 | Acc: 92.770% (9277/10000)\n",
            "\n",
            "Epoch: 33\n",
            "total train iters  391 | time: 41.163 sec Loss: 0.125 | Acc: 95.824% (47912/50000)\n",
            "total test iters  100 | time: 1494.293 sec Loss: 0.216 | Acc: 92.760% (9276/10000)\n",
            "\n",
            "Epoch: 34\n",
            "total train iters  391 | time: 41.161 sec Loss: 0.112 | Acc: 96.160% (48080/50000)\n",
            "total test iters  100 | time: 1538.219 sec Loss: 0.209 | Acc: 93.150% (9315/10000)\n",
            "\n",
            "Epoch: 35\n",
            "total train iters  391 | time: 41.214 sec Loss: 0.103 | Acc: 96.550% (48275/50000)\n",
            "total test iters  100 | time: 1582.191 sec Loss: 0.214 | Acc: 93.220% (9322/10000)\n",
            "\n",
            "Epoch: 36\n",
            "total train iters  391 | time: 41.201 sec Loss: 0.091 | Acc: 96.956% (48478/50000)\n",
            "total test iters  100 | time: 1626.117 sec Loss: 0.216 | Acc: 93.070% (9307/10000)\n",
            "\n",
            "Epoch: 37\n",
            "total train iters  391 | time: 41.223 sec Loss: 0.080 | Acc: 97.338% (48669/50000)\n",
            "total test iters  100 | time: 1670.095 sec Loss: 0.219 | Acc: 93.140% (9314/10000)\n",
            "\n",
            "Epoch: 38\n",
            "total train iters  391 | time: 41.242 sec Loss: 0.078 | Acc: 97.382% (48691/50000)\n",
            "total test iters  100 | time: 1714.078 sec Loss: 0.224 | Acc: 93.170% (9317/10000)\n",
            "\n",
            "Epoch: 39\n",
            "total train iters  391 | time: 41.168 sec Loss: 0.071 | Acc: 97.610% (48805/50000)\n",
            "total test iters  100 | time: 1757.975 sec Loss: 0.227 | Acc: 93.020% (9302/10000)\n",
            "\n",
            "Epoch: 40\n",
            "total train iters  391 | time: 41.118 sec Loss: 0.063 | Acc: 97.910% (48955/50000)\n",
            "total test iters  100 | time: 1801.830 sec Loss: 0.234 | Acc: 92.900% (9290/10000)\n",
            "\n",
            "Epoch: 41\n",
            "total train iters  391 | time: 41.104 sec Loss: 0.060 | Acc: 97.994% (48997/50000)\n",
            "total test iters  100 | time: 1845.698 sec Loss: 0.229 | Acc: 93.040% (9304/10000)\n",
            "\n",
            "Epoch: 42\n",
            "total train iters  391 | time: 41.060 sec Loss: 0.057 | Acc: 98.056% (49028/50000)\n",
            "total test iters  100 | time: 1889.506 sec Loss: 0.232 | Acc: 93.160% (9316/10000)\n",
            "\n",
            "Epoch: 43\n",
            "total train iters  391 | time: 41.067 sec Loss: 0.054 | Acc: 98.158% (49079/50000)\n",
            "total test iters  100 | time: 1933.354 sec Loss: 0.239 | Acc: 93.030% (9303/10000)\n",
            "\n",
            "Epoch: 44\n",
            "total train iters  391 | time: 41.031 sec Loss: 0.049 | Acc: 98.398% (49199/50000)\n",
            "total test iters  100 | time: 1977.123 sec Loss: 0.256 | Acc: 92.760% (9276/10000)\n",
            "\n",
            "Epoch: 45\n",
            "total train iters  391 | time: 41.011 sec Loss: 0.050 | Acc: 98.348% (49174/50000)\n",
            "total test iters  100 | time: 2020.924 sec Loss: 0.255 | Acc: 92.670% (9267/10000)\n",
            "\n",
            "Epoch: 46\n",
            "total train iters  391 | time: 41.162 sec Loss: 0.035 | Acc: 98.916% (49458/50000)\n",
            "total test iters  100 | time: 2064.894 sec Loss: 0.216 | Acc: 93.870% (9387/10000)\n",
            "\n",
            "Epoch: 47\n",
            "total train iters  391 | time: 41.164 sec Loss: 0.028 | Acc: 99.212% (49606/50000)\n",
            "total test iters  100 | time: 2108.865 sec Loss: 0.213 | Acc: 93.930% (9393/10000)\n",
            "\n",
            "Epoch: 48\n",
            "total train iters  391 | time: 41.176 sec Loss: 0.023 | Acc: 99.322% (49661/50000)\n",
            "total test iters  100 | time: 2152.778 sec Loss: 0.212 | Acc: 93.980% (9398/10000)\n",
            "\n",
            "Epoch: 49\n",
            "total train iters  391 | time: 41.040 sec Loss: 0.022 | Acc: 99.410% (49705/50000)\n",
            "total test iters  100 | time: 2196.598 sec Loss: 0.215 | Acc: 93.920% (9392/10000)\n",
            "\n",
            "Epoch: 50\n",
            "total train iters  391 | time: 41.092 sec Loss: 0.022 | Acc: 99.416% (49708/50000)\n",
            "total test iters  100 | time: 2240.420 sec Loss: 0.213 | Acc: 93.870% (9387/10000)\n",
            "\n",
            "Epoch: 51\n",
            "total train iters  391 | time: 41.047 sec Loss: 0.019 | Acc: 99.514% (49757/50000)\n",
            "total test iters  100 | time: 2284.230 sec Loss: 0.215 | Acc: 93.890% (9389/10000)\n",
            "\n",
            "Epoch: 52\n",
            "total train iters  391 | time: 41.062 sec Loss: 0.018 | Acc: 99.552% (49776/50000)\n",
            "total test iters  100 | time: 2328.100 sec Loss: 0.216 | Acc: 93.990% (9399/10000)\n",
            "\n",
            "Epoch: 53\n",
            "total train iters  391 | time: 41.174 sec Loss: 0.018 | Acc: 99.552% (49776/50000)\n",
            "total test iters  100 | time: 2372.025 sec Loss: 0.214 | Acc: 94.080% (9408/10000)\n",
            "\n",
            "Epoch: 54\n",
            "total train iters  391 | time: 41.218 sec Loss: 0.016 | Acc: 99.588% (49794/50000)\n",
            "total test iters  100 | time: 2415.964 sec Loss: 0.216 | Acc: 94.140% (9414/10000)\n",
            "\n",
            "Epoch: 55\n",
            "total train iters  391 | time: 41.198 sec Loss: 0.016 | Acc: 99.622% (49811/50000)\n",
            "total test iters  100 | time: 2459.940 sec Loss: 0.215 | Acc: 94.120% (9412/10000)\n",
            "\n",
            "Epoch: 56\n",
            "total train iters  391 | time: 41.127 sec Loss: 0.015 | Acc: 99.666% (49833/50000)\n",
            "total test iters  100 | time: 2503.808 sec Loss: 0.219 | Acc: 93.970% (9397/10000)\n",
            "\n",
            "Epoch: 57\n",
            "total train iters  391 | time: 41.054 sec Loss: 0.014 | Acc: 99.654% (49827/50000)\n",
            "total test iters  100 | time: 2547.658 sec Loss: 0.217 | Acc: 94.120% (9412/10000)\n",
            "\n",
            "Epoch: 58\n",
            "total train iters  391 | time: 40.993 sec Loss: 0.014 | Acc: 99.644% (49822/50000)\n",
            "total test iters  100 | time: 2591.382 sec Loss: 0.217 | Acc: 94.140% (9414/10000)\n",
            "\n",
            "Epoch: 59\n",
            "total train iters  391 | time: 41.023 sec Loss: 0.014 | Acc: 99.662% (49831/50000)\n",
            "total test iters  100 | time: 2635.119 sec Loss: 0.217 | Acc: 94.060% (9406/10000)\n",
            "\n",
            "Epoch: 60\n",
            "total train iters  391 | time: 41.055 sec Loss: 0.015 | Acc: 99.658% (49829/50000)\n",
            "total test iters  100 | time: 2678.914 sec Loss: 0.218 | Acc: 94.140% (9414/10000)\n",
            "\n",
            "Epoch: 61\n",
            "total train iters  391 | time: 41.068 sec Loss: 0.013 | Acc: 99.708% (49854/50000)\n",
            "total test iters  100 | time: 2722.708 sec Loss: 0.220 | Acc: 94.120% (9412/10000)\n",
            "\n",
            "Epoch: 62\n",
            "total train iters  391 | time: 41.120 sec Loss: 0.013 | Acc: 99.718% (49859/50000)\n",
            "total test iters  100 | time: 2766.620 sec Loss: 0.218 | Acc: 94.160% (9416/10000)\n",
            "\n",
            "Epoch: 63\n",
            "total train iters  391 | time: 41.149 sec Loss: 0.014 | Acc: 99.708% (49854/50000)\n",
            "total test iters  100 | time: 2810.487 sec Loss: 0.216 | Acc: 94.150% (9415/10000)\n",
            "\n",
            "Epoch: 64\n",
            "total train iters  391 | time: 41.236 sec Loss: 0.013 | Acc: 99.710% (49855/50000)\n",
            "total test iters  100 | time: 2854.509 sec Loss: 0.216 | Acc: 94.140% (9414/10000)\n",
            "\n",
            "Epoch: 65\n",
            "total train iters  391 | time: 41.227 sec Loss: 0.013 | Acc: 99.688% (49844/50000)\n",
            "total test iters  100 | time: 2898.489 sec Loss: 0.217 | Acc: 94.190% (9419/10000)\n",
            "\n",
            "Epoch: 66\n",
            "total train iters  391 | time: 41.155 sec Loss: 0.013 | Acc: 99.716% (49858/50000)\n",
            "total test iters  100 | time: 2942.380 sec Loss: 0.217 | Acc: 94.180% (9418/10000)\n",
            "\n",
            "Epoch: 67\n",
            "total train iters  391 | time: 41.110 sec Loss: 0.013 | Acc: 99.684% (49842/50000)\n",
            "total test iters  100 | time: 2986.224 sec Loss: 0.215 | Acc: 94.290% (9429/10000)\n",
            "\n",
            "Epoch: 68\n",
            "total train iters  391 | time: 41.077 sec Loss: 0.013 | Acc: 99.718% (49859/50000)\n",
            "total test iters  100 | time: 3030.059 sec Loss: 0.217 | Acc: 94.180% (9418/10000)\n",
            "\n",
            "Epoch: 69\n",
            "total train iters  391 | time: 41.097 sec Loss: 0.013 | Acc: 99.702% (49851/50000)\n",
            "total test iters  100 | time: 3073.928 sec Loss: 0.217 | Acc: 94.150% (9415/10000)\n",
            "\n",
            "Epoch: 70\n",
            "total train iters  391 | time: 41.059 sec Loss: 0.013 | Acc: 99.700% (49850/50000)\n",
            "total test iters  100 | time: 3117.719 sec Loss: 0.216 | Acc: 94.210% (9421/10000)\n",
            "\n",
            "Epoch: 71\n",
            "total train iters  391 | time: 41.116 sec Loss: 0.013 | Acc: 99.710% (49855/50000)\n",
            "total test iters  100 | time: 3161.551 sec Loss: 0.215 | Acc: 94.240% (9424/10000)\n",
            "\n",
            "Epoch: 72\n",
            "total train iters  391 | time: 41.164 sec Loss: 0.013 | Acc: 99.696% (49848/50000)\n",
            "total test iters  100 | time: 3205.462 sec Loss: 0.216 | Acc: 94.210% (9421/10000)\n",
            "\n",
            "Epoch: 73\n",
            "total train iters  391 | time: 41.113 sec Loss: 0.013 | Acc: 99.716% (49858/50000)\n",
            "total test iters  100 | time: 3249.328 sec Loss: 0.217 | Acc: 94.160% (9416/10000)\n",
            "\n",
            "Epoch: 74\n",
            "total train iters  391 | time: 41.161 sec Loss: 0.013 | Acc: 99.718% (49859/50000)\n",
            "total test iters  100 | time: 3293.289 sec Loss: 0.216 | Acc: 94.220% (9422/10000)\n",
            "\n",
            "Epoch: 75\n",
            "total train iters  391 | time: 41.164 sec Loss: 0.013 | Acc: 99.682% (49841/50000)\n",
            "total test iters  100 | time: 3337.171 sec Loss: 0.217 | Acc: 94.220% (9422/10000)\n",
            "\n",
            "Epoch: 76\n",
            "total train iters  391 | time: 41.214 sec Loss: 0.013 | Acc: 99.724% (49862/50000)\n",
            "total test iters  100 | time: 3381.116 sec Loss: 0.216 | Acc: 94.260% (9426/10000)\n",
            "\n",
            "Epoch: 77\n",
            "total train iters  391 | time: 41.201 sec Loss: 0.013 | Acc: 99.732% (49866/50000)\n",
            "total test iters  100 | time: 3425.128 sec Loss: 0.217 | Acc: 94.140% (9414/10000)\n",
            "\n",
            "Epoch: 78\n",
            "total train iters  391 | time: 41.211 sec Loss: 0.014 | Acc: 99.702% (49851/50000)\n",
            "total test iters  100 | time: 3469.060 sec Loss: 0.217 | Acc: 94.260% (9426/10000)\n",
            "\n",
            "Epoch: 79\n",
            "total train iters  391 | time: 41.170 sec Loss: 0.013 | Acc: 99.728% (49864/50000)\n",
            "total test iters  100 | time: 3512.988 sec Loss: 0.215 | Acc: 94.190% (9419/10000)\n",
            "\n",
            "Epoch: 80\n",
            "total train iters  391 | time: 41.076 sec Loss: 0.013 | Acc: 99.698% (49849/50000)\n",
            "total test iters  100 | time: 3556.834 sec Loss: 0.215 | Acc: 94.180% (9418/10000)\n",
            "\n",
            "Epoch: 81\n",
            "total train iters  391 | time: 41.075 sec Loss: 0.014 | Acc: 99.690% (49845/50000)\n",
            "total test iters  100 | time: 3600.723 sec Loss: 0.216 | Acc: 94.180% (9418/10000)\n",
            "\n",
            "Epoch: 82\n",
            "total train iters  391 | time: 41.124 sec Loss: 0.012 | Acc: 99.744% (49872/50000)\n",
            "total test iters  100 | time: 3644.589 sec Loss: 0.216 | Acc: 94.250% (9425/10000)\n",
            "\n",
            "Epoch: 83\n",
            "total train iters  391 | time: 41.148 sec Loss: 0.013 | Acc: 99.706% (49853/50000)\n",
            "total test iters  100 | time: 3688.479 sec Loss: 0.217 | Acc: 94.110% (9411/10000)\n",
            "\n",
            "Epoch: 84\n",
            "total train iters  391 | time: 41.177 sec Loss: 0.013 | Acc: 99.690% (49845/50000)\n",
            "total test iters  100 | time: 3732.390 sec Loss: 0.216 | Acc: 94.260% (9426/10000)\n",
            "\n",
            "Epoch: 85\n",
            "total train iters  391 | time: 41.204 sec Loss: 0.013 | Acc: 99.694% (49847/50000)\n",
            "total test iters  100 | time: 3776.329 sec Loss: 0.215 | Acc: 94.210% (9421/10000)\n",
            "\n",
            "Epoch: 86\n",
            "total train iters  391 | time: 41.217 sec Loss: 0.013 | Acc: 99.718% (49859/50000)\n",
            "total test iters  100 | time: 3820.307 sec Loss: 0.217 | Acc: 94.180% (9418/10000)\n",
            "\n",
            "Epoch: 87\n",
            "total train iters  391 | time: 41.183 sec Loss: 0.013 | Acc: 99.708% (49854/50000)\n",
            "total test iters  100 | time: 3864.259 sec Loss: 0.215 | Acc: 94.250% (9425/10000)\n",
            "\n",
            "Epoch: 88\n",
            "total train iters  391 | time: 41.121 sec Loss: 0.013 | Acc: 99.698% (49849/50000)\n",
            "total test iters  100 | time: 3908.141 sec Loss: 0.217 | Acc: 94.240% (9424/10000)\n",
            "\n",
            "Epoch: 89\n",
            "total train iters  391 | time: 41.064 sec Loss: 0.013 | Acc: 99.712% (49856/50000)\n",
            "total test iters  100 | time: 3951.952 sec Loss: 0.215 | Acc: 94.130% (9413/10000)\n",
            "\n",
            "Epoch: 90\n",
            "total train iters  391 | time: 40.994 sec Loss: 0.012 | Acc: 99.752% (49876/50000)\n",
            "total test iters  100 | time: 3995.681 sec Loss: 0.216 | Acc: 94.180% (9418/10000)\n",
            "\n",
            "Epoch: 91\n",
            "total train iters  391 | time: 41.039 sec Loss: 0.012 | Acc: 99.754% (49877/50000)\n",
            "total test iters  100 | time: 4039.441 sec Loss: 0.215 | Acc: 94.220% (9422/10000)\n",
            "\n",
            "Epoch: 92\n",
            "total train iters  391 | time: 41.087 sec Loss: 0.012 | Acc: 99.728% (49864/50000)\n",
            "total test iters  100 | time: 4083.256 sec Loss: 0.216 | Acc: 94.230% (9423/10000)\n",
            "\n",
            "Epoch: 93\n",
            "total train iters  391 | time: 41.184 sec Loss: 0.013 | Acc: 99.708% (49854/50000)\n",
            "total test iters  100 | time: 4127.249 sec Loss: 0.217 | Acc: 94.170% (9417/10000)\n",
            "\n",
            "Epoch: 94\n",
            "total train iters  391 | time: 41.172 sec Loss: 0.014 | Acc: 99.706% (49853/50000)\n",
            "total test iters  100 | time: 4171.168 sec Loss: 0.217 | Acc: 94.230% (9423/10000)\n",
            "\n",
            "Epoch: 95\n",
            "total train iters  391 | time: 41.202 sec Loss: 0.014 | Acc: 99.704% (49852/50000)\n",
            "total test iters  100 | time: 4215.161 sec Loss: 0.216 | Acc: 94.070% (9407/10000)\n",
            "\n",
            "Epoch: 96\n",
            "total train iters  391 | time: 41.252 sec Loss: 0.013 | Acc: 99.736% (49868/50000)\n",
            "total test iters  100 | time: 4259.187 sec Loss: 0.217 | Acc: 94.220% (9422/10000)\n",
            "\n",
            "Epoch: 97\n",
            "total train iters  391 | time: 41.141 sec Loss: 0.013 | Acc: 99.704% (49852/50000)\n",
            "total test iters  100 | time: 4303.051 sec Loss: 0.216 | Acc: 94.160% (9416/10000)\n",
            "\n",
            "Epoch: 98\n",
            "total train iters  391 | time: 41.037 sec Loss: 0.014 | Acc: 99.694% (49847/50000)\n",
            "total test iters  100 | time: 4346.860 sec Loss: 0.216 | Acc: 94.180% (9418/10000)\n",
            "\n",
            "Epoch: 99\n",
            "total train iters  391 | time: 41.004 sec Loss: 0.014 | Acc: 99.686% (49843/50000)\n",
            "total test iters  100 | time: 4390.679 sec Loss: 0.218 | Acc: 94.160% (9416/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p9i1XnhJ8AB",
        "colab_type": "text"
      },
      "source": [
        "#### Cyclincal learning rate with 2000 iteration per step\n",
        "Cyclincal learnging rate in a super convergence plicy. Using 2000 iterations per step, we are getting \n",
        "- **90% accuracy on validation data at 10th epoch**.\n",
        "- **validation accuracy of  93.830%** at **51st Epoch**.\n",
        "- And a maximum **validation accuracy of  94.410%** at **82nd Epoch**.\n",
        "\n",
        "We also ensure that **cyclic momentum** also works simultaniously, as LR increases and decreases, mometum decreases and increases. In pytorch this option is true by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-D2XwyfiDYwl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0bd51a8c-bbfa-45ca-dfe1-db9d045ca779"
      },
      "source": [
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=0.1,step_size_up=2000, step_size_down=None, mode='triangular')\n",
        "\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in net.parameters())\n",
        "pytorch_total_params_trainable = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
        "print(\"pytorch_total_params\",pytorch_total_params)\n",
        "print(\"pytorch_total_params_trainable\",pytorch_total_params_trainable)\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time = time.time()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print(\"total train iters \", len(trainloader), '| time: %.3f sec Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % ((time.time()-start_time), train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "start_time = time.time()\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "        print(\"total test iters \", len(testloader), '| time: %.3f sec Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % ((time.time()-start_time), test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    \n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+100):\n",
        "    train(epoch)\n",
        "    test(epoch)    "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pytorch_total_params 11173962\n",
            "pytorch_total_params_trainable 11173962\n",
            "\n",
            "Epoch: 0\n",
            "total train iters  391 | time: 39.969 sec Loss: 1.570 | Acc: 41.814% (20907/50000)\n",
            "total test iters  100 | time: 42.688 sec Loss: 1.220 | Acc: 56.360% (5636/10000)\n",
            "\n",
            "Epoch: 1\n",
            "total train iters  391 | time: 40.647 sec Loss: 1.006 | Acc: 64.150% (32075/50000)\n",
            "total test iters  100 | time: 86.092 sec Loss: 1.164 | Acc: 62.990% (6299/10000)\n",
            "\n",
            "Epoch: 2\n",
            "total train iters  391 | time: 40.981 sec Loss: 0.786 | Acc: 72.308% (36154/50000)\n",
            "total test iters  100 | time: 129.834 sec Loss: 0.759 | Acc: 73.680% (7368/10000)\n",
            "\n",
            "Epoch: 3\n",
            "total train iters  391 | time: 41.118 sec Loss: 0.657 | Acc: 77.186% (38593/50000)\n",
            "total test iters  100 | time: 173.695 sec Loss: 0.976 | Acc: 70.430% (7043/10000)\n",
            "\n",
            "Epoch: 4\n",
            "total train iters  391 | time: 41.187 sec Loss: 0.579 | Acc: 79.912% (39956/50000)\n",
            "total test iters  100 | time: 217.609 sec Loss: 0.894 | Acc: 72.590% (7259/10000)\n",
            "\n",
            "Epoch: 5\n",
            "total train iters  391 | time: 41.175 sec Loss: 0.511 | Acc: 82.450% (41225/50000)\n",
            "total test iters  100 | time: 261.553 sec Loss: 0.819 | Acc: 73.080% (7308/10000)\n",
            "\n",
            "Epoch: 6\n",
            "total train iters  391 | time: 41.255 sec Loss: 0.443 | Acc: 84.786% (42393/50000)\n",
            "total test iters  100 | time: 305.557 sec Loss: 0.494 | Acc: 83.430% (8343/10000)\n",
            "\n",
            "Epoch: 7\n",
            "total train iters  391 | time: 41.253 sec Loss: 0.385 | Acc: 86.658% (43329/50000)\n",
            "total test iters  100 | time: 349.582 sec Loss: 0.615 | Acc: 80.460% (8046/10000)\n",
            "\n",
            "Epoch: 8\n",
            "total train iters  391 | time: 41.318 sec Loss: 0.321 | Acc: 88.994% (44497/50000)\n",
            "total test iters  100 | time: 393.631 sec Loss: 0.367 | Acc: 87.550% (8755/10000)\n",
            "\n",
            "Epoch: 9\n",
            "total train iters  391 | time: 41.384 sec Loss: 0.253 | Acc: 91.364% (45682/50000)\n",
            "total test iters  100 | time: 437.740 sec Loss: 0.302 | Acc: 90.010% (9001/10000)\n",
            "\n",
            "Epoch: 10\n",
            "total train iters  391 | time: 41.458 sec Loss: 0.202 | Acc: 93.132% (46566/50000)\n",
            "total test iters  100 | time: 481.931 sec Loss: 0.352 | Acc: 88.560% (8856/10000)\n",
            "\n",
            "Epoch: 11\n",
            "total train iters  391 | time: 41.321 sec Loss: 0.264 | Acc: 90.932% (45466/50000)\n",
            "total test iters  100 | time: 525.988 sec Loss: 0.584 | Acc: 81.570% (8157/10000)\n",
            "\n",
            "Epoch: 12\n",
            "total train iters  391 | time: 41.318 sec Loss: 0.315 | Acc: 89.054% (44527/50000)\n",
            "total test iters  100 | time: 570.026 sec Loss: 0.496 | Acc: 83.370% (8337/10000)\n",
            "\n",
            "Epoch: 13\n",
            "total train iters  391 | time: 41.212 sec Loss: 0.343 | Acc: 88.266% (44133/50000)\n",
            "total test iters  100 | time: 613.954 sec Loss: 0.583 | Acc: 81.180% (8118/10000)\n",
            "\n",
            "Epoch: 14\n",
            "total train iters  391 | time: 41.203 sec Loss: 0.355 | Acc: 87.548% (43774/50000)\n",
            "total test iters  100 | time: 657.911 sec Loss: 0.495 | Acc: 83.620% (8362/10000)\n",
            "\n",
            "Epoch: 15\n",
            "total train iters  391 | time: 41.223 sec Loss: 0.353 | Acc: 87.822% (43911/50000)\n",
            "total test iters  100 | time: 701.968 sec Loss: 0.462 | Acc: 84.350% (8435/10000)\n",
            "\n",
            "Epoch: 16\n",
            "total train iters  391 | time: 41.187 sec Loss: 0.314 | Acc: 89.140% (44570/50000)\n",
            "total test iters  100 | time: 745.885 sec Loss: 0.491 | Acc: 84.120% (8412/10000)\n",
            "\n",
            "Epoch: 17\n",
            "total train iters  391 | time: 41.186 sec Loss: 0.273 | Acc: 90.670% (45335/50000)\n",
            "total test iters  100 | time: 789.785 sec Loss: 0.342 | Acc: 88.560% (8856/10000)\n",
            "\n",
            "Epoch: 18\n",
            "total train iters  391 | time: 41.207 sec Loss: 0.224 | Acc: 92.306% (46153/50000)\n",
            "total test iters  100 | time: 833.758 sec Loss: 0.316 | Acc: 89.590% (8959/10000)\n",
            "\n",
            "Epoch: 19\n",
            "total train iters  391 | time: 41.269 sec Loss: 0.164 | Acc: 94.276% (47138/50000)\n",
            "total test iters  100 | time: 877.769 sec Loss: 0.250 | Acc: 91.640% (9164/10000)\n",
            "\n",
            "Epoch: 20\n",
            "total train iters  391 | time: 41.333 sec Loss: 0.107 | Acc: 96.440% (48220/50000)\n",
            "total test iters  100 | time: 921.841 sec Loss: 0.251 | Acc: 91.750% (9175/10000)\n",
            "\n",
            "Epoch: 21\n",
            "total train iters  391 | time: 41.229 sec Loss: 0.143 | Acc: 95.162% (47581/50000)\n",
            "total test iters  100 | time: 965.879 sec Loss: 0.404 | Acc: 87.400% (8740/10000)\n",
            "\n",
            "Epoch: 22\n",
            "total train iters  391 | time: 41.220 sec Loss: 0.222 | Acc: 92.370% (46185/50000)\n",
            "total test iters  100 | time: 1009.818 sec Loss: 0.485 | Acc: 84.670% (8467/10000)\n",
            "\n",
            "Epoch: 23\n",
            "total train iters  391 | time: 41.155 sec Loss: 0.270 | Acc: 90.576% (45288/50000)\n",
            "total test iters  100 | time: 1053.683 sec Loss: 0.697 | Acc: 77.490% (7749/10000)\n",
            "\n",
            "Epoch: 24\n",
            "total train iters  391 | time: 41.189 sec Loss: 0.294 | Acc: 89.848% (44924/50000)\n",
            "total test iters  100 | time: 1097.605 sec Loss: 0.424 | Acc: 85.650% (8565/10000)\n",
            "\n",
            "Epoch: 25\n",
            "total train iters  391 | time: 41.156 sec Loss: 0.300 | Acc: 89.698% (44849/50000)\n",
            "total test iters  100 | time: 1141.487 sec Loss: 0.541 | Acc: 82.260% (8226/10000)\n",
            "\n",
            "Epoch: 26\n",
            "total train iters  391 | time: 41.076 sec Loss: 0.272 | Acc: 90.672% (45336/50000)\n",
            "total test iters  100 | time: 1185.325 sec Loss: 0.431 | Acc: 85.950% (8595/10000)\n",
            "\n",
            "Epoch: 27\n",
            "total train iters  391 | time: 41.140 sec Loss: 0.233 | Acc: 91.884% (45942/50000)\n",
            "total test iters  100 | time: 1229.204 sec Loss: 0.420 | Acc: 86.410% (8641/10000)\n",
            "\n",
            "Epoch: 28\n",
            "total train iters  391 | time: 41.260 sec Loss: 0.191 | Acc: 93.430% (46715/50000)\n",
            "total test iters  100 | time: 1273.172 sec Loss: 0.305 | Acc: 90.320% (9032/10000)\n",
            "\n",
            "Epoch: 29\n",
            "total train iters  391 | time: 41.312 sec Loss: 0.134 | Acc: 95.416% (47708/50000)\n",
            "total test iters  100 | time: 1317.218 sec Loss: 0.243 | Acc: 92.390% (9239/10000)\n",
            "\n",
            "Epoch: 30\n",
            "total train iters  391 | time: 41.212 sec Loss: 0.075 | Acc: 97.502% (48751/50000)\n",
            "total test iters  100 | time: 1361.189 sec Loss: 0.225 | Acc: 92.990% (9299/10000)\n",
            "\n",
            "Epoch: 31\n",
            "total train iters  391 | time: 41.211 sec Loss: 0.087 | Acc: 97.130% (48565/50000)\n",
            "total test iters  100 | time: 1405.128 sec Loss: 0.297 | Acc: 91.200% (9120/10000)\n",
            "\n",
            "Epoch: 32\n",
            "total train iters  391 | time: 41.196 sec Loss: 0.167 | Acc: 94.136% (47068/50000)\n",
            "total test iters  100 | time: 1449.054 sec Loss: 0.351 | Acc: 88.900% (8890/10000)\n",
            "\n",
            "Epoch: 33\n",
            "total train iters  391 | time: 41.217 sec Loss: 0.228 | Acc: 92.172% (46086/50000)\n",
            "total test iters  100 | time: 1492.994 sec Loss: 0.448 | Acc: 85.840% (8584/10000)\n",
            "\n",
            "Epoch: 34\n",
            "total train iters  391 | time: 41.184 sec Loss: 0.257 | Acc: 91.114% (45557/50000)\n",
            "total test iters  100 | time: 1536.882 sec Loss: 0.515 | Acc: 84.420% (8442/10000)\n",
            "\n",
            "Epoch: 35\n",
            "total train iters  391 | time: 41.170 sec Loss: 0.269 | Acc: 90.752% (45376/50000)\n",
            "total test iters  100 | time: 1580.783 sec Loss: 0.503 | Acc: 84.420% (8442/10000)\n",
            "\n",
            "Epoch: 36\n",
            "total train iters  391 | time: 41.178 sec Loss: 0.254 | Acc: 91.308% (45654/50000)\n",
            "total test iters  100 | time: 1624.716 sec Loss: 0.456 | Acc: 85.970% (8597/10000)\n",
            "\n",
            "Epoch: 37\n",
            "total train iters  391 | time: 41.108 sec Loss: 0.217 | Acc: 92.498% (46249/50000)\n",
            "total test iters  100 | time: 1668.606 sec Loss: 0.309 | Acc: 89.660% (8966/10000)\n",
            "\n",
            "Epoch: 38\n",
            "total train iters  391 | time: 41.228 sec Loss: 0.173 | Acc: 94.054% (47027/50000)\n",
            "total test iters  100 | time: 1712.543 sec Loss: 0.302 | Acc: 90.160% (9016/10000)\n",
            "\n",
            "Epoch: 39\n",
            "total train iters  391 | time: 41.235 sec Loss: 0.122 | Acc: 95.820% (47910/50000)\n",
            "total test iters  100 | time: 1756.495 sec Loss: 0.253 | Acc: 92.320% (9232/10000)\n",
            "\n",
            "Epoch: 40\n",
            "total train iters  391 | time: 41.230 sec Loss: 0.068 | Acc: 97.830% (48915/50000)\n",
            "total test iters  100 | time: 1800.501 sec Loss: 0.210 | Acc: 93.480% (9348/10000)\n",
            "\n",
            "Epoch: 41\n",
            "total train iters  391 | time: 41.201 sec Loss: 0.055 | Acc: 98.204% (49102/50000)\n",
            "total test iters  100 | time: 1844.421 sec Loss: 0.270 | Acc: 92.080% (9208/10000)\n",
            "\n",
            "Epoch: 42\n",
            "total train iters  391 | time: 41.189 sec Loss: 0.124 | Acc: 95.778% (47889/50000)\n",
            "total test iters  100 | time: 1888.375 sec Loss: 0.467 | Acc: 86.120% (8612/10000)\n",
            "\n",
            "Epoch: 43\n",
            "total train iters  391 | time: 41.192 sec Loss: 0.195 | Acc: 93.262% (46631/50000)\n",
            "total test iters  100 | time: 1932.315 sec Loss: 0.575 | Acc: 83.320% (8332/10000)\n",
            "\n",
            "Epoch: 44\n",
            "total train iters  391 | time: 41.184 sec Loss: 0.237 | Acc: 91.772% (45886/50000)\n",
            "total test iters  100 | time: 1976.279 sec Loss: 0.555 | Acc: 82.800% (8280/10000)\n",
            "\n",
            "Epoch: 45\n",
            "total train iters  391 | time: 41.253 sec Loss: 0.253 | Acc: 91.354% (45677/50000)\n",
            "total test iters  100 | time: 2020.254 sec Loss: 0.396 | Acc: 87.480% (8748/10000)\n",
            "\n",
            "Epoch: 46\n",
            "total train iters  391 | time: 41.220 sec Loss: 0.247 | Acc: 91.460% (45730/50000)\n",
            "total test iters  100 | time: 2064.254 sec Loss: 0.426 | Acc: 86.070% (8607/10000)\n",
            "\n",
            "Epoch: 47\n",
            "total train iters  391 | time: 41.244 sec Loss: 0.206 | Acc: 92.882% (46441/50000)\n",
            "total test iters  100 | time: 2108.234 sec Loss: 0.352 | Acc: 88.910% (8891/10000)\n",
            "\n",
            "Epoch: 48\n",
            "total train iters  391 | time: 41.226 sec Loss: 0.173 | Acc: 94.104% (47052/50000)\n",
            "total test iters  100 | time: 2152.179 sec Loss: 0.298 | Acc: 90.470% (9047/10000)\n",
            "\n",
            "Epoch: 49\n",
            "total train iters  391 | time: 41.169 sec Loss: 0.120 | Acc: 95.880% (47940/50000)\n",
            "total test iters  100 | time: 2196.075 sec Loss: 0.256 | Acc: 91.760% (9176/10000)\n",
            "\n",
            "Epoch: 50\n",
            "total train iters  391 | time: 41.119 sec Loss: 0.067 | Acc: 97.778% (48889/50000)\n",
            "total test iters  100 | time: 2239.911 sec Loss: 0.197 | Acc: 93.830% (9383/10000)\n",
            "\n",
            "Epoch: 51\n",
            "total train iters  391 | time: 41.182 sec Loss: 0.042 | Acc: 98.770% (49385/50000)\n",
            "total test iters  100 | time: 2283.876 sec Loss: 0.245 | Acc: 92.480% (9248/10000)\n",
            "\n",
            "Epoch: 52\n",
            "total train iters  391 | time: 41.271 sec Loss: 0.092 | Acc: 96.922% (48461/50000)\n",
            "total test iters  100 | time: 2327.859 sec Loss: 0.365 | Acc: 88.880% (8888/10000)\n",
            "\n",
            "Epoch: 53\n",
            "total train iters  391 | time: 41.244 sec Loss: 0.170 | Acc: 94.104% (47052/50000)\n",
            "total test iters  100 | time: 2371.841 sec Loss: 0.502 | Acc: 84.570% (8457/10000)\n",
            "\n",
            "Epoch: 54\n",
            "total train iters  391 | time: 41.124 sec Loss: 0.211 | Acc: 92.732% (46366/50000)\n",
            "total test iters  100 | time: 2415.745 sec Loss: 0.525 | Acc: 83.860% (8386/10000)\n",
            "\n",
            "Epoch: 55\n",
            "total train iters  391 | time: 41.102 sec Loss: 0.246 | Acc: 91.554% (45777/50000)\n",
            "total test iters  100 | time: 2459.578 sec Loss: 0.483 | Acc: 84.270% (8427/10000)\n",
            "\n",
            "Epoch: 56\n",
            "total train iters  391 | time: 41.100 sec Loss: 0.241 | Acc: 91.646% (45823/50000)\n",
            "total test iters  100 | time: 2503.398 sec Loss: 0.456 | Acc: 85.720% (8572/10000)\n",
            "\n",
            "Epoch: 57\n",
            "total train iters  391 | time: 41.083 sec Loss: 0.207 | Acc: 92.912% (46456/50000)\n",
            "total test iters  100 | time: 2547.198 sec Loss: 0.376 | Acc: 87.600% (8760/10000)\n",
            "\n",
            "Epoch: 58\n",
            "total train iters  391 | time: 41.108 sec Loss: 0.167 | Acc: 94.342% (47171/50000)\n",
            "total test iters  100 | time: 2591.011 sec Loss: 0.321 | Acc: 89.940% (8994/10000)\n",
            "\n",
            "Epoch: 59\n",
            "total train iters  391 | time: 41.201 sec Loss: 0.123 | Acc: 95.784% (47892/50000)\n",
            "total test iters  100 | time: 2634.922 sec Loss: 0.250 | Acc: 92.190% (9219/10000)\n",
            "\n",
            "Epoch: 60\n",
            "total train iters  391 | time: 41.168 sec Loss: 0.069 | Acc: 97.738% (48869/50000)\n",
            "total test iters  100 | time: 2678.875 sec Loss: 0.224 | Acc: 93.000% (9300/10000)\n",
            "\n",
            "Epoch: 61\n",
            "total train iters  391 | time: 41.186 sec Loss: 0.036 | Acc: 98.910% (49455/50000)\n",
            "total test iters  100 | time: 2722.769 sec Loss: 0.211 | Acc: 93.450% (9345/10000)\n",
            "\n",
            "Epoch: 62\n",
            "total train iters  391 | time: 41.162 sec Loss: 0.061 | Acc: 97.944% (48972/50000)\n",
            "total test iters  100 | time: 2766.663 sec Loss: 0.309 | Acc: 91.320% (9132/10000)\n",
            "\n",
            "Epoch: 63\n",
            "total train iters  391 | time: 41.102 sec Loss: 0.143 | Acc: 94.988% (47494/50000)\n",
            "total test iters  100 | time: 2810.473 sec Loss: 0.386 | Acc: 88.060% (8806/10000)\n",
            "\n",
            "Epoch: 64\n",
            "total train iters  391 | time: 41.070 sec Loss: 0.199 | Acc: 93.078% (46539/50000)\n",
            "total test iters  100 | time: 2854.235 sec Loss: 0.469 | Acc: 85.180% (8518/10000)\n",
            "\n",
            "Epoch: 65\n",
            "total train iters  391 | time: 41.085 sec Loss: 0.226 | Acc: 92.236% (46118/50000)\n",
            "total test iters  100 | time: 2898.067 sec Loss: 0.546 | Acc: 83.180% (8318/10000)\n",
            "\n",
            "Epoch: 66\n",
            "total train iters  391 | time: 41.159 sec Loss: 0.240 | Acc: 91.782% (45891/50000)\n",
            "total test iters  100 | time: 2941.984 sec Loss: 0.652 | Acc: 81.440% (8144/10000)\n",
            "\n",
            "Epoch: 67\n",
            "total train iters  391 | time: 41.214 sec Loss: 0.203 | Acc: 93.108% (46554/50000)\n",
            "total test iters  100 | time: 2985.907 sec Loss: 0.336 | Acc: 89.200% (8920/10000)\n",
            "\n",
            "Epoch: 68\n",
            "total train iters  391 | time: 41.234 sec Loss: 0.172 | Acc: 94.138% (47069/50000)\n",
            "total test iters  100 | time: 3029.882 sec Loss: 0.328 | Acc: 89.460% (8946/10000)\n",
            "\n",
            "Epoch: 69\n",
            "total train iters  391 | time: 41.219 sec Loss: 0.123 | Acc: 95.842% (47921/50000)\n",
            "total test iters  100 | time: 3073.826 sec Loss: 0.268 | Acc: 91.680% (9168/10000)\n",
            "\n",
            "Epoch: 70\n",
            "total train iters  391 | time: 41.163 sec Loss: 0.076 | Acc: 97.494% (48747/50000)\n",
            "total test iters  100 | time: 3117.739 sec Loss: 0.202 | Acc: 93.740% (9374/10000)\n",
            "\n",
            "Epoch: 71\n",
            "total train iters  391 | time: 41.142 sec Loss: 0.037 | Acc: 98.884% (49442/50000)\n",
            "total test iters  100 | time: 3161.614 sec Loss: 0.182 | Acc: 94.320% (9432/10000)\n",
            "\n",
            "Epoch: 72\n",
            "total train iters  391 | time: 41.123 sec Loss: 0.046 | Acc: 98.436% (49218/50000)\n",
            "total test iters  100 | time: 3205.439 sec Loss: 0.338 | Acc: 90.420% (9042/10000)\n",
            "\n",
            "Epoch: 73\n",
            "total train iters  391 | time: 41.085 sec Loss: 0.115 | Acc: 96.014% (48007/50000)\n",
            "total test iters  100 | time: 3249.304 sec Loss: 0.381 | Acc: 88.230% (8823/10000)\n",
            "\n",
            "Epoch: 74\n",
            "total train iters  391 | time: 41.138 sec Loss: 0.180 | Acc: 93.936% (46968/50000)\n",
            "total test iters  100 | time: 3293.151 sec Loss: 0.452 | Acc: 86.340% (8634/10000)\n",
            "\n",
            "Epoch: 75\n",
            "total train iters  391 | time: 41.091 sec Loss: 0.220 | Acc: 92.500% (46250/50000)\n",
            "total test iters  100 | time: 3336.979 sec Loss: 0.445 | Acc: 85.670% (8567/10000)\n",
            "\n",
            "Epoch: 76\n",
            "total train iters  391 | time: 41.134 sec Loss: 0.228 | Acc: 92.202% (46101/50000)\n",
            "total test iters  100 | time: 3380.893 sec Loss: 0.538 | Acc: 83.210% (8321/10000)\n",
            "\n",
            "Epoch: 77\n",
            "total train iters  391 | time: 41.073 sec Loss: 0.211 | Acc: 92.712% (46356/50000)\n",
            "total test iters  100 | time: 3424.691 sec Loss: 0.357 | Acc: 88.170% (8817/10000)\n",
            "\n",
            "Epoch: 78\n",
            "total train iters  391 | time: 41.123 sec Loss: 0.172 | Acc: 94.090% (47045/50000)\n",
            "total test iters  100 | time: 3468.556 sec Loss: 0.316 | Acc: 89.660% (8966/10000)\n",
            "\n",
            "Epoch: 79\n",
            "total train iters  391 | time: 41.178 sec Loss: 0.132 | Acc: 95.466% (47733/50000)\n",
            "total test iters  100 | time: 3512.507 sec Loss: 0.264 | Acc: 91.620% (9162/10000)\n",
            "\n",
            "Epoch: 80\n",
            "total train iters  391 | time: 41.220 sec Loss: 0.088 | Acc: 97.024% (48512/50000)\n",
            "total test iters  100 | time: 3556.480 sec Loss: 0.240 | Acc: 92.950% (9295/10000)\n",
            "\n",
            "Epoch: 81\n",
            "total train iters  391 | time: 41.224 sec Loss: 0.040 | Acc: 98.740% (49370/50000)\n",
            "total test iters  100 | time: 3600.455 sec Loss: 0.184 | Acc: 94.410% (9441/10000)\n",
            "\n",
            "Epoch: 82\n",
            "total train iters  391 | time: 41.226 sec Loss: 0.035 | Acc: 98.820% (49410/50000)\n",
            "total test iters  100 | time: 3644.411 sec Loss: 0.251 | Acc: 92.670% (9267/10000)\n",
            "\n",
            "Epoch: 83\n",
            "total train iters  391 | time: 41.218 sec Loss: 0.093 | Acc: 96.900% (48450/50000)\n",
            "total test iters  100 | time: 3688.364 sec Loss: 0.515 | Acc: 85.600% (8560/10000)\n",
            "\n",
            "Epoch: 84\n",
            "total train iters  391 | time: 41.154 sec Loss: 0.162 | Acc: 94.454% (47227/50000)\n",
            "total test iters  100 | time: 3732.260 sec Loss: 0.369 | Acc: 88.680% (8868/10000)\n",
            "\n",
            "Epoch: 85\n",
            "total train iters  391 | time: 41.148 sec Loss: 0.210 | Acc: 92.732% (46366/50000)\n",
            "total test iters  100 | time: 3776.143 sec Loss: 0.630 | Acc: 81.800% (8180/10000)\n",
            "\n",
            "Epoch: 86\n",
            "total train iters  391 | time: 41.154 sec Loss: 0.226 | Acc: 92.280% (46140/50000)\n",
            "total test iters  100 | time: 3820.021 sec Loss: 0.585 | Acc: 81.600% (8160/10000)\n",
            "\n",
            "Epoch: 87\n",
            "total train iters  391 | time: 41.077 sec Loss: 0.212 | Acc: 92.772% (46386/50000)\n",
            "total test iters  100 | time: 3863.882 sec Loss: 0.353 | Acc: 88.590% (8859/10000)\n",
            "\n",
            "Epoch: 88\n",
            "total train iters  391 | time: 41.122 sec Loss: 0.175 | Acc: 94.060% (47030/50000)\n",
            "total test iters  100 | time: 3907.804 sec Loss: 0.321 | Acc: 89.590% (8959/10000)\n",
            "\n",
            "Epoch: 89\n",
            "total train iters  391 | time: 41.110 sec Loss: 0.135 | Acc: 95.336% (47668/50000)\n",
            "total test iters  100 | time: 3951.689 sec Loss: 0.337 | Acc: 89.420% (8942/10000)\n",
            "\n",
            "Epoch: 90\n",
            "total train iters  391 | time: 41.286 sec Loss: 0.090 | Acc: 96.958% (48479/50000)\n",
            "total test iters  100 | time: 3995.695 sec Loss: 0.236 | Acc: 93.050% (9305/10000)\n",
            "\n",
            "Epoch: 91\n",
            "total train iters  391 | time: 41.149 sec Loss: 0.041 | Acc: 98.720% (49360/50000)\n",
            "total test iters  100 | time: 4039.632 sec Loss: 0.196 | Acc: 94.130% (9413/10000)\n",
            "\n",
            "Epoch: 92\n",
            "total train iters  391 | time: 41.124 sec Loss: 0.028 | Acc: 99.162% (49581/50000)\n",
            "total test iters  100 | time: 4083.501 sec Loss: 0.220 | Acc: 93.700% (9370/10000)\n",
            "\n",
            "Epoch: 93\n",
            "total train iters  391 | time: 41.112 sec Loss: 0.069 | Acc: 97.614% (48807/50000)\n",
            "total test iters  100 | time: 4127.332 sec Loss: 0.338 | Acc: 89.870% (8987/10000)\n",
            "\n",
            "Epoch: 94\n",
            "total train iters  391 | time: 41.127 sec Loss: 0.145 | Acc: 95.054% (47527/50000)\n",
            "total test iters  100 | time: 4171.203 sec Loss: 0.430 | Acc: 87.080% (8708/10000)\n",
            "\n",
            "Epoch: 95\n",
            "total train iters  391 | time: 41.149 sec Loss: 0.196 | Acc: 93.320% (46660/50000)\n",
            "total test iters  100 | time: 4215.068 sec Loss: 0.397 | Acc: 87.400% (8740/10000)\n",
            "\n",
            "Epoch: 96\n",
            "total train iters  391 | time: 41.237 sec Loss: 0.216 | Acc: 92.650% (46325/50000)\n",
            "total test iters  100 | time: 4259.091 sec Loss: 0.443 | Acc: 86.170% (8617/10000)\n",
            "\n",
            "Epoch: 97\n",
            "total train iters  391 | time: 41.168 sec Loss: 0.216 | Acc: 92.658% (46329/50000)\n",
            "total test iters  100 | time: 4302.997 sec Loss: 0.465 | Acc: 85.500% (8550/10000)\n",
            "\n",
            "Epoch: 98\n",
            "total train iters  391 | time: 41.155 sec Loss: 0.176 | Acc: 93.912% (46956/50000)\n",
            "total test iters  100 | time: 4346.893 sec Loss: 0.336 | Acc: 89.370% (8937/10000)\n",
            "\n",
            "Epoch: 99\n",
            "total train iters  391 | time: 41.087 sec Loss: 0.142 | Acc: 95.182% (47591/50000)\n",
            "total test iters  100 | time: 4390.773 sec Loss: 0.372 | Acc: 88.610% (8861/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHS8l4q2KjVz",
        "colab_type": "text"
      },
      "source": [
        "#### Cyclincal learning rate with 10000 iteration per step\n",
        "Cyclincal learnging rate in a super convergence plicy. Using 10000 iterations per step, we are getting **Val Acc: 94.360% accuracy on validation data at 52nd epoch**. Here we crossed 94% human benchmark at quite ealry compared to CLR with step size of 2000, at 81st epoch.\n",
        "But it took 42 epochs to reach an val accuracy of 90%, which was attained in 9th epoch with small step size cycles. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ed3HK33T_M_I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6389ec36-7943-4007-9887-36f0f1c34f1b"
      },
      "source": [
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = CyclicLR(optimizer, base_lr=0.0001, max_lr=0.1,step_size_up=10000, step_size_down=10000, mode='triangular')\n",
        "\n",
        "\n",
        "pytorch_total_params = sum(p.numel() for p in net.parameters())\n",
        "pytorch_total_params_trainable = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
        "print(\"pytorch_total_params\",pytorch_total_params)\n",
        "print(\"pytorch_total_params_trainable\",pytorch_total_params_trainable)\n",
        "\n",
        "\n",
        "# Training\n",
        "def train(epoch):\n",
        "    print('\\nEpoch: %d' % epoch)\n",
        "    net.train()\n",
        "    train_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    start_time = time.time()\n",
        "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    print(\"total train iters \", len(trainloader), '| time: %.3f sec Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % ((time.time()-start_time), train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "start_time = time.time()\n",
        "def test(epoch):\n",
        "    global best_acc\n",
        "    net.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "            \n",
        "        print(\"total test iters \", len(testloader), '| time: %.3f sec Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
        "        % ((time.time()-start_time), test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
        "\n",
        "    \n",
        "\n",
        "for epoch in range(start_epoch, start_epoch+100):\n",
        "    train(epoch)\n",
        "    test(epoch)    "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "pytorch_total_params 11173962\n",
            "pytorch_total_params_trainable 11173962\n",
            "\n",
            "Epoch: 0\n",
            "total train iters  391 | time: 41.249 sec Loss: 1.743 | Acc: 34.648% (17324/50000)\n",
            "total test iters  100 | time: 44.038 sec Loss: 1.333 | Acc: 51.410% (5141/10000)\n",
            "\n",
            "Epoch: 1\n",
            "total train iters  391 | time: 41.677 sec Loss: 1.149 | Acc: 58.546% (29273/50000)\n",
            "total test iters  100 | time: 88.454 sec Loss: 1.124 | Acc: 61.090% (6109/10000)\n",
            "\n",
            "Epoch: 2\n",
            "total train iters  391 | time: 41.393 sec Loss: 0.862 | Acc: 69.476% (34738/50000)\n",
            "total test iters  100 | time: 132.599 sec Loss: 0.903 | Acc: 70.450% (7045/10000)\n",
            "\n",
            "Epoch: 3\n",
            "total train iters  391 | time: 41.542 sec Loss: 0.702 | Acc: 75.276% (37638/50000)\n",
            "total test iters  100 | time: 176.914 sec Loss: 0.763 | Acc: 74.350% (7435/10000)\n",
            "\n",
            "Epoch: 4\n",
            "total train iters  391 | time: 41.406 sec Loss: 0.612 | Acc: 78.758% (39379/50000)\n",
            "total test iters  100 | time: 221.112 sec Loss: 0.719 | Acc: 76.390% (7639/10000)\n",
            "\n",
            "Epoch: 5\n",
            "total train iters  391 | time: 41.573 sec Loss: 0.550 | Acc: 80.918% (40459/50000)\n",
            "total test iters  100 | time: 265.436 sec Loss: 0.646 | Acc: 78.750% (7875/10000)\n",
            "\n",
            "Epoch: 6\n",
            "total train iters  391 | time: 41.572 sec Loss: 0.500 | Acc: 82.698% (41349/50000)\n",
            "total test iters  100 | time: 309.835 sec Loss: 0.632 | Acc: 78.800% (7880/10000)\n",
            "\n",
            "Epoch: 7\n",
            "total train iters  391 | time: 41.523 sec Loss: 0.463 | Acc: 83.926% (41963/50000)\n",
            "total test iters  100 | time: 354.107 sec Loss: 0.554 | Acc: 81.980% (8198/10000)\n",
            "\n",
            "Epoch: 8\n",
            "total train iters  391 | time: 41.421 sec Loss: 0.427 | Acc: 85.328% (42664/50000)\n",
            "total test iters  100 | time: 398.320 sec Loss: 0.657 | Acc: 78.700% (7870/10000)\n",
            "\n",
            "Epoch: 9\n",
            "total train iters  391 | time: 41.382 sec Loss: 0.411 | Acc: 85.878% (42939/50000)\n",
            "total test iters  100 | time: 442.491 sec Loss: 0.529 | Acc: 82.590% (8259/10000)\n",
            "\n",
            "Epoch: 10\n",
            "total train iters  391 | time: 41.363 sec Loss: 0.381 | Acc: 86.840% (43420/50000)\n",
            "total test iters  100 | time: 486.655 sec Loss: 0.504 | Acc: 83.090% (8309/10000)\n",
            "\n",
            "Epoch: 11\n",
            "total train iters  391 | time: 41.375 sec Loss: 0.371 | Acc: 87.204% (43602/50000)\n",
            "total test iters  100 | time: 530.822 sec Loss: 0.464 | Acc: 84.510% (8451/10000)\n",
            "\n",
            "Epoch: 12\n",
            "total train iters  391 | time: 41.454 sec Loss: 0.348 | Acc: 87.812% (43906/50000)\n",
            "total test iters  100 | time: 575.072 sec Loss: 0.452 | Acc: 85.320% (8532/10000)\n",
            "\n",
            "Epoch: 13\n",
            "total train iters  391 | time: 41.473 sec Loss: 0.334 | Acc: 88.512% (44256/50000)\n",
            "total test iters  100 | time: 619.270 sec Loss: 0.457 | Acc: 84.820% (8482/10000)\n",
            "\n",
            "Epoch: 14\n",
            "total train iters  391 | time: 41.483 sec Loss: 0.324 | Acc: 89.014% (44507/50000)\n",
            "total test iters  100 | time: 663.480 sec Loss: 0.500 | Acc: 83.660% (8366/10000)\n",
            "\n",
            "Epoch: 15\n",
            "total train iters  391 | time: 41.438 sec Loss: 0.318 | Acc: 88.900% (44450/50000)\n",
            "total test iters  100 | time: 707.653 sec Loss: 0.432 | Acc: 85.820% (8582/10000)\n",
            "\n",
            "Epoch: 16\n",
            "total train iters  391 | time: 41.393 sec Loss: 0.313 | Acc: 89.088% (44544/50000)\n",
            "total test iters  100 | time: 751.824 sec Loss: 0.419 | Acc: 86.190% (8619/10000)\n",
            "\n",
            "Epoch: 17\n",
            "total train iters  391 | time: 41.394 sec Loss: 0.304 | Acc: 89.550% (44775/50000)\n",
            "total test iters  100 | time: 795.966 sec Loss: 0.445 | Acc: 85.410% (8541/10000)\n",
            "\n",
            "Epoch: 18\n",
            "total train iters  391 | time: 41.424 sec Loss: 0.303 | Acc: 89.544% (44772/50000)\n",
            "total test iters  100 | time: 840.112 sec Loss: 0.439 | Acc: 85.420% (8542/10000)\n",
            "\n",
            "Epoch: 19\n",
            "total train iters  391 | time: 41.425 sec Loss: 0.291 | Acc: 89.982% (44991/50000)\n",
            "total test iters  100 | time: 884.311 sec Loss: 0.434 | Acc: 85.680% (8568/10000)\n",
            "\n",
            "Epoch: 20\n",
            "total train iters  391 | time: 41.407 sec Loss: 0.290 | Acc: 89.990% (44995/50000)\n",
            "total test iters  100 | time: 928.524 sec Loss: 0.468 | Acc: 84.820% (8482/10000)\n",
            "\n",
            "Epoch: 21\n",
            "total train iters  391 | time: 41.351 sec Loss: 0.289 | Acc: 90.206% (45103/50000)\n",
            "total test iters  100 | time: 972.607 sec Loss: 0.465 | Acc: 85.080% (8508/10000)\n",
            "\n",
            "Epoch: 22\n",
            "total train iters  391 | time: 41.340 sec Loss: 0.289 | Acc: 90.086% (45043/50000)\n",
            "total test iters  100 | time: 1016.666 sec Loss: 0.470 | Acc: 84.680% (8468/10000)\n",
            "\n",
            "Epoch: 23\n",
            "total train iters  391 | time: 41.299 sec Loss: 0.282 | Acc: 90.334% (45167/50000)\n",
            "total test iters  100 | time: 1060.724 sec Loss: 0.496 | Acc: 84.110% (8411/10000)\n",
            "\n",
            "Epoch: 24\n",
            "total train iters  391 | time: 41.372 sec Loss: 0.285 | Acc: 90.184% (45092/50000)\n",
            "total test iters  100 | time: 1104.830 sec Loss: 0.448 | Acc: 85.240% (8524/10000)\n",
            "\n",
            "Epoch: 25\n",
            "total train iters  391 | time: 41.427 sec Loss: 0.277 | Acc: 90.438% (45219/50000)\n",
            "total test iters  100 | time: 1148.995 sec Loss: 0.570 | Acc: 82.350% (8235/10000)\n",
            "\n",
            "Epoch: 26\n",
            "total train iters  391 | time: 41.368 sec Loss: 0.272 | Acc: 90.666% (45333/50000)\n",
            "total test iters  100 | time: 1193.172 sec Loss: 0.626 | Acc: 81.440% (8144/10000)\n",
            "\n",
            "Epoch: 27\n",
            "total train iters  391 | time: 41.308 sec Loss: 0.262 | Acc: 91.158% (45579/50000)\n",
            "total test iters  100 | time: 1237.201 sec Loss: 0.496 | Acc: 84.140% (8414/10000)\n",
            "\n",
            "Epoch: 28\n",
            "total train iters  391 | time: 41.323 sec Loss: 0.255 | Acc: 91.200% (45600/50000)\n",
            "total test iters  100 | time: 1281.286 sec Loss: 0.507 | Acc: 84.220% (8422/10000)\n",
            "\n",
            "Epoch: 29\n",
            "total train iters  391 | time: 41.324 sec Loss: 0.250 | Acc: 91.540% (45770/50000)\n",
            "total test iters  100 | time: 1325.401 sec Loss: 0.467 | Acc: 84.990% (8499/10000)\n",
            "\n",
            "Epoch: 30\n",
            "total train iters  391 | time: 41.381 sec Loss: 0.237 | Acc: 91.802% (45901/50000)\n",
            "total test iters  100 | time: 1369.539 sec Loss: 0.531 | Acc: 84.420% (8442/10000)\n",
            "\n",
            "Epoch: 31\n",
            "total train iters  391 | time: 41.336 sec Loss: 0.236 | Acc: 91.914% (45957/50000)\n",
            "total test iters  100 | time: 1413.659 sec Loss: 0.526 | Acc: 83.540% (8354/10000)\n",
            "\n",
            "Epoch: 32\n",
            "total train iters  391 | time: 41.274 sec Loss: 0.222 | Acc: 92.330% (46165/50000)\n",
            "total test iters  100 | time: 1457.644 sec Loss: 0.366 | Acc: 87.940% (8794/10000)\n",
            "\n",
            "Epoch: 33\n",
            "total train iters  391 | time: 41.224 sec Loss: 0.215 | Acc: 92.608% (46304/50000)\n",
            "total test iters  100 | time: 1501.584 sec Loss: 0.357 | Acc: 88.470% (8847/10000)\n",
            "\n",
            "Epoch: 34\n",
            "total train iters  391 | time: 41.148 sec Loss: 0.210 | Acc: 92.846% (46423/50000)\n",
            "total test iters  100 | time: 1545.529 sec Loss: 0.367 | Acc: 88.310% (8831/10000)\n",
            "\n",
            "Epoch: 35\n",
            "total train iters  391 | time: 41.170 sec Loss: 0.199 | Acc: 93.210% (46605/50000)\n",
            "total test iters  100 | time: 1589.468 sec Loss: 0.364 | Acc: 88.600% (8860/10000)\n",
            "\n",
            "Epoch: 36\n",
            "total train iters  391 | time: 41.278 sec Loss: 0.195 | Acc: 93.204% (46602/50000)\n",
            "total test iters  100 | time: 1633.514 sec Loss: 0.373 | Acc: 88.640% (8864/10000)\n",
            "\n",
            "Epoch: 37\n",
            "total train iters  391 | time: 41.321 sec Loss: 0.183 | Acc: 93.794% (46897/50000)\n",
            "total test iters  100 | time: 1677.590 sec Loss: 0.373 | Acc: 88.500% (8850/10000)\n",
            "\n",
            "Epoch: 38\n",
            "total train iters  391 | time: 41.273 sec Loss: 0.175 | Acc: 94.014% (47007/50000)\n",
            "total test iters  100 | time: 1721.635 sec Loss: 0.386 | Acc: 88.030% (8803/10000)\n",
            "\n",
            "Epoch: 39\n",
            "total train iters  391 | time: 41.231 sec Loss: 0.164 | Acc: 94.442% (47221/50000)\n",
            "total test iters  100 | time: 1765.606 sec Loss: 0.390 | Acc: 87.930% (8793/10000)\n",
            "\n",
            "Epoch: 40\n",
            "total train iters  391 | time: 41.247 sec Loss: 0.154 | Acc: 94.706% (47353/50000)\n",
            "total test iters  100 | time: 1809.572 sec Loss: 0.366 | Acc: 88.830% (8883/10000)\n",
            "\n",
            "Epoch: 41\n",
            "total train iters  391 | time: 41.317 sec Loss: 0.144 | Acc: 95.030% (47515/50000)\n",
            "total test iters  100 | time: 1853.620 sec Loss: 0.275 | Acc: 91.150% (9115/10000)\n",
            "\n",
            "Epoch: 42\n",
            "total train iters  391 | time: 41.322 sec Loss: 0.130 | Acc: 95.602% (47801/50000)\n",
            "total test iters  100 | time: 1897.667 sec Loss: 0.294 | Acc: 90.810% (9081/10000)\n",
            "\n",
            "Epoch: 43\n",
            "total train iters  391 | time: 41.268 sec Loss: 0.120 | Acc: 95.884% (47942/50000)\n",
            "total test iters  100 | time: 1941.724 sec Loss: 0.299 | Acc: 90.830% (9083/10000)\n",
            "\n",
            "Epoch: 44\n",
            "total train iters  391 | time: 41.237 sec Loss: 0.107 | Acc: 96.356% (48178/50000)\n",
            "total test iters  100 | time: 1985.693 sec Loss: 0.274 | Acc: 91.520% (9152/10000)\n",
            "\n",
            "Epoch: 45\n",
            "total train iters  391 | time: 41.261 sec Loss: 0.089 | Acc: 96.908% (48454/50000)\n",
            "total test iters  100 | time: 2029.695 sec Loss: 0.248 | Acc: 92.440% (9244/10000)\n",
            "\n",
            "Epoch: 46\n",
            "total train iters  391 | time: 41.335 sec Loss: 0.068 | Acc: 97.730% (48865/50000)\n",
            "total test iters  100 | time: 2073.745 sec Loss: 0.239 | Acc: 92.960% (9296/10000)\n",
            "\n",
            "Epoch: 47\n",
            "total train iters  391 | time: 41.293 sec Loss: 0.052 | Acc: 98.318% (49159/50000)\n",
            "total test iters  100 | time: 2117.772 sec Loss: 0.240 | Acc: 93.330% (9333/10000)\n",
            "\n",
            "Epoch: 48\n",
            "total train iters  391 | time: 41.242 sec Loss: 0.035 | Acc: 98.876% (49438/50000)\n",
            "total test iters  100 | time: 2161.742 sec Loss: 0.228 | Acc: 93.690% (9369/10000)\n",
            "\n",
            "Epoch: 49\n",
            "total train iters  391 | time: 41.187 sec Loss: 0.024 | Acc: 99.248% (49624/50000)\n",
            "total test iters  100 | time: 2205.687 sec Loss: 0.211 | Acc: 94.300% (9430/10000)\n",
            "\n",
            "Epoch: 50\n",
            "total train iters  391 | time: 41.298 sec Loss: 0.017 | Acc: 99.534% (49767/50000)\n",
            "total test iters  100 | time: 2249.723 sec Loss: 0.203 | Acc: 94.320% (9432/10000)\n",
            "\n",
            "Epoch: 51\n",
            "total train iters  391 | time: 41.288 sec Loss: 0.012 | Acc: 99.718% (49859/50000)\n",
            "total test iters  100 | time: 2293.732 sec Loss: 0.205 | Acc: 94.360% (9436/10000)\n",
            "\n",
            "Epoch: 52\n",
            "total train iters  391 | time: 41.341 sec Loss: 0.014 | Acc: 99.612% (49806/50000)\n",
            "total test iters  100 | time: 2337.807 sec Loss: 0.217 | Acc: 93.940% (9394/10000)\n",
            "\n",
            "Epoch: 53\n",
            "total train iters  391 | time: 41.251 sec Loss: 0.019 | Acc: 99.384% (49692/50000)\n",
            "total test iters  100 | time: 2381.805 sec Loss: 0.256 | Acc: 93.220% (9322/10000)\n",
            "\n",
            "Epoch: 54\n",
            "total train iters  391 | time: 41.218 sec Loss: 0.042 | Acc: 98.608% (49304/50000)\n",
            "total test iters  100 | time: 2425.804 sec Loss: 0.307 | Acc: 91.250% (9125/10000)\n",
            "\n",
            "Epoch: 55\n",
            "total train iters  391 | time: 41.310 sec Loss: 0.072 | Acc: 97.538% (48769/50000)\n",
            "total test iters  100 | time: 2469.852 sec Loss: 0.310 | Acc: 90.860% (9086/10000)\n",
            "\n",
            "Epoch: 56\n",
            "total train iters  391 | time: 41.354 sec Loss: 0.103 | Acc: 96.472% (48236/50000)\n",
            "total test iters  100 | time: 2513.943 sec Loss: 0.336 | Acc: 90.210% (9021/10000)\n",
            "\n",
            "Epoch: 57\n",
            "total train iters  391 | time: 41.200 sec Loss: 0.129 | Acc: 95.598% (47799/50000)\n",
            "total test iters  100 | time: 2557.858 sec Loss: 0.379 | Acc: 88.530% (8853/10000)\n",
            "\n",
            "Epoch: 58\n",
            "total train iters  391 | time: 41.164 sec Loss: 0.150 | Acc: 94.864% (47432/50000)\n",
            "total test iters  100 | time: 2601.808 sec Loss: 0.389 | Acc: 87.950% (8795/10000)\n",
            "\n",
            "Epoch: 59\n",
            "total train iters  391 | time: 41.149 sec Loss: 0.163 | Acc: 94.348% (47174/50000)\n",
            "total test iters  100 | time: 2645.673 sec Loss: 0.360 | Acc: 88.660% (8866/10000)\n",
            "\n",
            "Epoch: 60\n",
            "total train iters  391 | time: 41.115 sec Loss: 0.168 | Acc: 94.286% (47143/50000)\n",
            "total test iters  100 | time: 2689.508 sec Loss: 0.360 | Acc: 89.000% (8900/10000)\n",
            "\n",
            "Epoch: 61\n",
            "total train iters  391 | time: 41.064 sec Loss: 0.173 | Acc: 94.052% (47026/50000)\n",
            "total test iters  100 | time: 2733.289 sec Loss: 0.489 | Acc: 85.470% (8547/10000)\n",
            "\n",
            "Epoch: 62\n",
            "total train iters  391 | time: 41.150 sec Loss: 0.185 | Acc: 93.650% (46825/50000)\n",
            "total test iters  100 | time: 2777.153 sec Loss: 0.326 | Acc: 89.230% (8923/10000)\n",
            "\n",
            "Epoch: 63\n",
            "total train iters  391 | time: 41.138 sec Loss: 0.179 | Acc: 93.894% (46947/50000)\n",
            "total test iters  100 | time: 2821.015 sec Loss: 0.470 | Acc: 85.430% (8543/10000)\n",
            "\n",
            "Epoch: 64\n",
            "total train iters  391 | time: 41.141 sec Loss: 0.191 | Acc: 93.424% (46712/50000)\n",
            "total test iters  100 | time: 2864.877 sec Loss: 0.356 | Acc: 88.440% (8844/10000)\n",
            "\n",
            "Epoch: 65\n",
            "total train iters  391 | time: 41.264 sec Loss: 0.188 | Acc: 93.558% (46779/50000)\n",
            "total test iters  100 | time: 2908.884 sec Loss: 0.341 | Acc: 89.200% (8920/10000)\n",
            "\n",
            "Epoch: 66\n",
            "total train iters  391 | time: 41.212 sec Loss: 0.194 | Acc: 93.400% (46700/50000)\n",
            "total test iters  100 | time: 2952.804 sec Loss: 0.373 | Acc: 88.290% (8829/10000)\n",
            "\n",
            "Epoch: 67\n",
            "total train iters  391 | time: 41.134 sec Loss: 0.194 | Acc: 93.466% (46733/50000)\n",
            "total test iters  100 | time: 2996.640 sec Loss: 0.396 | Acc: 87.580% (8758/10000)\n",
            "\n",
            "Epoch: 68\n",
            "total train iters  391 | time: 41.107 sec Loss: 0.199 | Acc: 93.308% (46654/50000)\n",
            "total test iters  100 | time: 3040.460 sec Loss: 0.355 | Acc: 88.680% (8868/10000)\n",
            "\n",
            "Epoch: 69\n",
            "total train iters  391 | time: 41.143 sec Loss: 0.202 | Acc: 93.050% (46525/50000)\n",
            "total test iters  100 | time: 3084.372 sec Loss: 0.398 | Acc: 87.480% (8748/10000)\n",
            "\n",
            "Epoch: 70\n",
            "total train iters  391 | time: 41.087 sec Loss: 0.201 | Acc: 93.124% (46562/50000)\n",
            "total test iters  100 | time: 3128.166 sec Loss: 0.405 | Acc: 87.390% (8739/10000)\n",
            "\n",
            "Epoch: 71\n",
            "total train iters  391 | time: 41.170 sec Loss: 0.203 | Acc: 93.122% (46561/50000)\n",
            "total test iters  100 | time: 3172.104 sec Loss: 0.326 | Acc: 89.320% (8932/10000)\n",
            "\n",
            "Epoch: 72\n",
            "total train iters  391 | time: 41.156 sec Loss: 0.208 | Acc: 92.916% (46458/50000)\n",
            "total test iters  100 | time: 3215.996 sec Loss: 0.372 | Acc: 88.610% (8861/10000)\n",
            "\n",
            "Epoch: 73\n",
            "total train iters  391 | time: 41.153 sec Loss: 0.208 | Acc: 92.804% (46402/50000)\n",
            "total test iters  100 | time: 3259.866 sec Loss: 0.458 | Acc: 86.080% (8608/10000)\n",
            "\n",
            "Epoch: 74\n",
            "total train iters  391 | time: 41.157 sec Loss: 0.210 | Acc: 92.730% (46365/50000)\n",
            "total test iters  100 | time: 3303.813 sec Loss: 0.369 | Acc: 88.280% (8828/10000)\n",
            "\n",
            "Epoch: 75\n",
            "total train iters  391 | time: 41.215 sec Loss: 0.211 | Acc: 92.770% (46385/50000)\n",
            "total test iters  100 | time: 3347.730 sec Loss: 0.356 | Acc: 88.520% (8852/10000)\n",
            "\n",
            "Epoch: 76\n",
            "total train iters  391 | time: 41.206 sec Loss: 0.212 | Acc: 92.744% (46372/50000)\n",
            "total test iters  100 | time: 3391.663 sec Loss: 0.507 | Acc: 84.360% (8436/10000)\n",
            "\n",
            "Epoch: 77\n",
            "total train iters  391 | time: 41.233 sec Loss: 0.211 | Acc: 92.882% (46441/50000)\n",
            "total test iters  100 | time: 3435.618 sec Loss: 0.408 | Acc: 86.840% (8684/10000)\n",
            "\n",
            "Epoch: 78\n",
            "total train iters  391 | time: 41.211 sec Loss: 0.200 | Acc: 93.126% (46563/50000)\n",
            "total test iters  100 | time: 3479.535 sec Loss: 0.400 | Acc: 87.170% (8717/10000)\n",
            "\n",
            "Epoch: 79\n",
            "total train iters  391 | time: 41.140 sec Loss: 0.191 | Acc: 93.394% (46697/50000)\n",
            "total test iters  100 | time: 3523.421 sec Loss: 0.416 | Acc: 86.850% (8685/10000)\n",
            "\n",
            "Epoch: 80\n",
            "total train iters  391 | time: 41.135 sec Loss: 0.186 | Acc: 93.670% (46835/50000)\n",
            "total test iters  100 | time: 3567.289 sec Loss: 0.307 | Acc: 89.820% (8982/10000)\n",
            "\n",
            "Epoch: 81\n",
            "total train iters  391 | time: 41.152 sec Loss: 0.180 | Acc: 93.886% (46943/50000)\n",
            "total test iters  100 | time: 3611.153 sec Loss: 0.335 | Acc: 89.370% (8937/10000)\n",
            "\n",
            "Epoch: 82\n",
            "total train iters  391 | time: 41.098 sec Loss: 0.181 | Acc: 93.852% (46926/50000)\n",
            "total test iters  100 | time: 3654.961 sec Loss: 0.334 | Acc: 89.420% (8942/10000)\n",
            "\n",
            "Epoch: 83\n",
            "total train iters  391 | time: 41.194 sec Loss: 0.172 | Acc: 94.032% (47016/50000)\n",
            "total test iters  100 | time: 3698.887 sec Loss: 0.392 | Acc: 88.160% (8816/10000)\n",
            "\n",
            "Epoch: 84\n",
            "total train iters  391 | time: 41.247 sec Loss: 0.167 | Acc: 94.338% (47169/50000)\n",
            "total test iters  100 | time: 3742.869 sec Loss: 0.405 | Acc: 87.840% (8784/10000)\n",
            "\n",
            "Epoch: 85\n",
            "total train iters  391 | time: 41.239 sec Loss: 0.161 | Acc: 94.520% (47260/50000)\n",
            "total test iters  100 | time: 3786.820 sec Loss: 0.310 | Acc: 90.430% (9043/10000)\n",
            "\n",
            "Epoch: 86\n",
            "total train iters  391 | time: 41.233 sec Loss: 0.161 | Acc: 94.454% (47227/50000)\n",
            "total test iters  100 | time: 3830.773 sec Loss: 0.386 | Acc: 88.450% (8845/10000)\n",
            "\n",
            "Epoch: 87\n",
            "total train iters  391 | time: 41.196 sec Loss: 0.149 | Acc: 94.862% (47431/50000)\n",
            "total test iters  100 | time: 3874.690 sec Loss: 0.306 | Acc: 90.240% (9024/10000)\n",
            "\n",
            "Epoch: 88\n",
            "total train iters  391 | time: 41.203 sec Loss: 0.145 | Acc: 94.948% (47474/50000)\n",
            "total test iters  100 | time: 3918.647 sec Loss: 0.325 | Acc: 89.670% (8967/10000)\n",
            "\n",
            "Epoch: 89\n",
            "total train iters  391 | time: 41.193 sec Loss: 0.134 | Acc: 95.432% (47716/50000)\n",
            "total test iters  100 | time: 3962.562 sec Loss: 0.306 | Acc: 90.450% (9045/10000)\n",
            "\n",
            "Epoch: 90\n",
            "total train iters  391 | time: 41.147 sec Loss: 0.130 | Acc: 95.472% (47736/50000)\n",
            "total test iters  100 | time: 4006.417 sec Loss: 0.354 | Acc: 89.240% (8924/10000)\n",
            "\n",
            "Epoch: 91\n",
            "total train iters  391 | time: 41.152 sec Loss: 0.116 | Acc: 96.114% (48057/50000)\n",
            "total test iters  100 | time: 4050.324 sec Loss: 0.278 | Acc: 91.370% (9137/10000)\n",
            "\n",
            "Epoch: 92\n",
            "total train iters  391 | time: 41.174 sec Loss: 0.106 | Acc: 96.394% (48197/50000)\n",
            "total test iters  100 | time: 4094.204 sec Loss: 0.345 | Acc: 89.780% (8978/10000)\n",
            "\n",
            "Epoch: 93\n",
            "total train iters  391 | time: 41.171 sec Loss: 0.101 | Acc: 96.582% (48291/50000)\n",
            "total test iters  100 | time: 4138.180 sec Loss: 0.278 | Acc: 91.390% (9139/10000)\n",
            "\n",
            "Epoch: 94\n",
            "total train iters  391 | time: 41.117 sec Loss: 0.089 | Acc: 97.002% (48501/50000)\n",
            "total test iters  100 | time: 4182.068 sec Loss: 0.303 | Acc: 90.640% (9064/10000)\n",
            "\n",
            "Epoch: 95\n",
            "total train iters  391 | time: 41.181 sec Loss: 0.074 | Acc: 97.496% (48748/50000)\n",
            "total test iters  100 | time: 4226.048 sec Loss: 0.257 | Acc: 92.500% (9250/10000)\n",
            "\n",
            "Epoch: 96\n",
            "total train iters  391 | time: 41.228 sec Loss: 0.062 | Acc: 97.900% (48950/50000)\n",
            "total test iters  100 | time: 4270.026 sec Loss: 0.263 | Acc: 92.570% (9257/10000)\n",
            "\n",
            "Epoch: 97\n",
            "total train iters  391 | time: 41.168 sec Loss: 0.052 | Acc: 98.224% (49112/50000)\n",
            "total test iters  100 | time: 4313.937 sec Loss: 0.229 | Acc: 93.460% (9346/10000)\n",
            "\n",
            "Epoch: 98\n",
            "total train iters  391 | time: 41.167 sec Loss: 0.036 | Acc: 98.812% (49406/50000)\n",
            "total test iters  100 | time: 4357.846 sec Loss: 0.257 | Acc: 92.960% (9296/10000)\n",
            "\n",
            "Epoch: 99\n",
            "total train iters  391 | time: 41.139 sec Loss: 0.025 | Acc: 99.224% (49612/50000)\n",
            "total test iters  100 | time: 4401.766 sec Loss: 0.216 | Acc: 93.980% (9398/10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGkORIVgn3l9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}